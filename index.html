<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Tahira Shehzadi</title>
    <meta name="description" content="Academic website of Tahira Shehzadi">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@400;500&display=swap">
    <link rel="stylesheet" href="style.css">
    <link rel="icon" type="image/png" href="icons/favicon.png">
  </head>
  <body>
    <div class="menu">
      <a href="#home">Home</a> &nbsp;&nbsp;&nbsp;
      <a href="#bio">Bio</a> &nbsp;&nbsp;&nbsp;
      <a href="#projects">Projects</a> &nbsp;&nbsp;&nbsp;
      <a href="#publications">Publications</a> &nbsp;&nbsp;&nbsp;
      <a href="#services">Services/Honors</a>
    </div>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="home">
              <td style="padding:20px;width:67%;vertical-align:middle">
                <span class="name">Tahira Shehzadi</span>
                <p class="information">
                  Machine Learning Engineer at <a href="https://av.dfki.de/">DFKI</a>
                  and Ph.D. Candidate at <a href="https://rptu.de/">RPTU Kaiserslautern</a>
                </p>
                <p class="information">
                  <a href="mailto:tahirashehzadi9@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="CV-Tahira.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=-QzlQU8AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/tahira_31">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/tahirashehzadi">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/tahira45/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:20px;width:33%;vertical-align:middle">
                <img class="dp" src="figures/Tahira.jpeg" alt="Profile photo">
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="bio">
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="section">About Me</span>
                <p class="bio">
                  I am a Machine Learning Engineer at the German Research Center for Artificial Intelligence (<a href="https://av.dfki.de/">DFKI</a>) and a PhD Candidate in Computer Science at RPTU Kaiserslautern–Landau, supervised by Prof. Dr. Didier Stricker. My research advances robust and efficient visual perception, with a focus on semi-supervised learning, document intelligence, and object detection in challenging and low-annotation environments. My work has resulted in first-author publications at premier venues including CVPR, ICCV and ICDAR.
                  </p><p class="bio">
                  At DFKI, I lead research across multiple large-scale applied AI projects including semi-supervised object detection for complex 2D environments, a high-precision multimodal system for document layout analysis under the LUMINOUS project, and medical anomaly detection pipelines for the AIRISE initiative. I also completed an Applied AI Scientist internship at Tensorlake, San Francisco, where I worked on document intelligence systems, led VLM integration for document analysis, and developed data generation and post-processing pipelines for improving document layout, OCR accuracy, and strikethrough detection.
                  My research focuses on multimodal perception, data-efficient learning, and transformer architectures, with a long-term goal of enabling agentic AI systems that can autonomously understand, reason, and act based on visual information.
                  </p>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="projects">
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="section">Projects</span>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/luminous.gif" alt="Open Vocabulary Egocentric Scene Understanding">
                        <br>
                        <em><b>Prompt:</b> Make a Pasta</em>
                      </td>
                      <td>
                        <papertitle>Open Vocabulary Egocentric Scene Understanding</papertitle>
                        This project leverages multi-modal foundation models to enhance egocentric scene understanding in unconstrained environments. The system enables natural language-driven task execution through LLM prompting, allowing users to perform complex sequences of actions by simply describing tasks like <i>make a pasta</i>. This approach combines advanced vision-language models with practical augmented reality applications.
                      </td>
                    </tr>
                  </table>
                </div>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="publications">
              <td style="padding:0 20px 20px 20px;width:100%;vertical-align:middle">
                <span class="section">Selected Publications <a href="https://scholar.google.com/citations?user=0CN11t4AAAAJ&hl=en">[Google Scholar]</a></span>

                 <div class="paper" style="background-color: #ffffd0">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/STEP-DETR.png" alt="STEP-DETR">
                      </td>
                      <td>
                        <papertitle>STEP-DETR: Advancing DETR-based Semi-Supervised Object Detection with Super Teacher and Pseudo-Label Guided Text Queries</papertitle>
                        <strong>T. Shehzadi</strong>, K.A. Hashmi, S. Sarode, D. Stricker, M.Z. Afzal
                        <br>
                        <em>ICCV</em>, 2025
                        <br>
                        <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Shehzadi_STEP-DETR_Advancing_DETR-based_Semi-Supervised_Object_Detection_with_Super_Teacher_and_ICCV_2025_paper.pdf">pdf</a> /
                        <a href="https://openaccess.thecvf.com/content/ICCV2025/supplemental/Shehzadi_STEP-DETR_Advancing_DETR-based_ICCV_2025_supplemental.pdf">supp</a>
                      </td>
                    </tr>
                  </table>
                </div>
                
                <div class="paper" style="background-color: #ffffd0">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/sparse_semidetr.jpeg" alt="Sparse Semi-DETR">
                      </td>
                      <td>
                        <papertitle>Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection</papertitle>
                        <strong>T. Shehzadi</strong>, K.A. Hashmi, D.Stricker, M.Z. Afzal
                        <br>
                        <em>CVPR</em>, 2024
                        <br>
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Shehzadi_Sparse_Semi-DETR_Sparse_Learnable_Queries_for_Semi-Supervised_Object_Detection_CVPR_2024_paper.pdf">pdf</a> /
                        <a href="https://www.youtube.com/watch?v=lCjCveKC8ys&ab_channel=TahiraShehzadi">video</a>
                      </td>
                    </tr>
                  </table>
                </div>
               

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/transformer_OD_survey.png" alt="Transformer OD Survey">
                      </td>
                      <td>
                        <papertitle>Object Detection with Transformers: A Review</papertitle>
                        <strong>T. Shehzadi </strong>, K.A. Hashmi, D.Stricker, M.Z. Afzal
                        <br>
                        <em>Sensors</em>, 2025
                        <br>
                        <a href="https://www.mdpi.com/1424-8220/25/19/6025">pdf</a> /
                        <a href="https://github.com/mindgarage-shan/transformer_object_detection_survey">code</a>
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/SemiTabDETR.png" alt="SemiTabDETR">
                      </td>
                      <td>
                        <papertitle>SemiTabDETR: End-to-End Semi-Supervised Table Detection with Transformer-based Enhanced Query Approach</papertitle>
                        <strong>T. Shehzadi </strong>, D.Stricker, M.Z. Afzal
                        <br>
                        <em>ICDAR</em>, 2025
                        <br>
                        <a href="https://link.springer.com/chapter/10.1007/978-3-032-04614-7_15">pdf</a> 
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/Additive.png" alt="Additive Attention">
                      </td>
                      <td>
                        <papertitle>Efficient Additive Attention for Transformer-based Semi-supervised Document Layout Analysis</papertitle>
                        <strong>T. Shehzadi </strong>, I. Ifza, D.Stricker, M.Z. Afzal
                        <br>
                        <em>ICCV Workshop</em>, 2025
                        <br>
                        <a href="https://openaccess.thecvf.com/content/ICCV2025W/WiCV/papers/Shehzadi_Efficient_Additive_Attention_for_Transformer-based_Semi-supervised_Document_Layout_Analysis_ICCVW_2025_paper.pdf">pdf</a> 
                      </td>
                    </tr>
                  </table>
                </div>


                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/ClassroomKD.png" alt="ClassroomKD">
                      </td>
                      <td>
                        <papertitle>Classroom-Inspired Multi-mentor Distillation with Adaptive Learning Strategies</papertitle>
                        S. Sarode, M.S. Khan,<strong>T. Shehzadi </strong>, D.Stricker, M.Z. Afzal
                        <br>
                        <em>IntelliSys</em>, 2025
                        <br>
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-99965-9_19">pdf</a> 
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/DocSemi.png" alt="DocSemi">
                      </td>
                      <td>
                        <papertitle>DocSemi: Efficient Document Layout Analysis with Guided Queries</papertitle>
                        <strong>T. Shehzadi </strong>, I. Ifza, D.Stricker, M.Z. Afzal
                        <br>
                        <em>ICCV Workshop</em>, 2025
                        <br>
                        <a href="https://openaccess.thecvf.com/content/ICCV2025W/VisionDocs/papers/Shehzadi_DocSemi_Efficient_Document_Layout_Analysis_with_Guided_Queries_ICCVW_2025_paper.pdf">pdf</a> 
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/FD-SSD.png" alt="FD-SSD">
                      </td>
                      <td>
                        <papertitle>FD-SSD: Semi-supervised Detection of Bone Fenestration and Dehiscence in Intraoral Images</papertitle>
                         <strong>T. Shehzadi </strong>, I. Ifza, D.Stricker, M.Z. Afzal
                        <br>
                        <em>MIUA</em>, 2025
                        <br>
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-98691-8_1">pdf</a> 
                      </td>
                    </tr>
                  </table>
                </div>


                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/BankCheck.png" alt="BankCheck">
                      </td>
                      <td>
                        <papertitle>Enhanced Bank Check Security: Introducing a Novel Dataset and Transformer-Based Approach for Detection and Verification</papertitle>
                        M.S. Khan*,<strong>T. Shehzadi* </strong>, R. Noor, D.Stricker, M.Z. Afzal
                        <br>
                        <em>ICDAR Workshop</em>, 2024
                        <br>
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-70442-0_3">pdf</a> 
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/UnSupDLA.png" alt="UnSupDLA">
                      </td>
                      <td>
                        <papertitle>UnSupDLA: Towards Unsupervised Document Layout Analysis</papertitle>
                        T. Sheikh*,<strong>T. Shehzadi* </strong>, R. Noor, D.Stricker, M.Z. Afzal
                        <br>
                        <em>ICDAR Workshop</em>, 2024
                        <br>
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-70442-0_9">pdf</a> 
                      </td>
                    </tr>
                  </table>
                </div>

                <div class="paper">
                  <table>
                    <tr>
                      <td>
                        <img src="figures/IJDAR24.png" alt="IJDAR">
                      </td>
                      <td>
                        <papertitle>End-to-End Semi-Supervised approach with Modulated Object Queries for Table Detection in Documents</papertitle>
                        Iqra. Ehsan,<strong>T. Shehzadi </strong>, D.Stricker, M.Z. Afzal
                        <br>
                        <em>IJDAR</em>, 2024
                        <br>
                        <a href="https://link.springer.com/article/10.1007/s10032-024-00471-0">pdf</a> 
                      </td>
                    </tr>
                  </table>
                </div>


              


          


            

              
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr id="services">
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="section">Services</span>
                <p class="bio">
                  <strong>Reviewer of Conferences:</strong><br>
                  ICDAR2025, MIUA2025, WACV2025
                </p>
                <p class="bio">
                  <strong>Reviewer of Journals:</strong><br>
                  IEEE Access<br>
                  Springer Nature
                
                </p>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="section">Honors & Awards</span>
                <ul>
                  <li>NSF Travel Grant for WiML Workshop at NeurIPS 2024</li>
                  <li>Received DAAD PhD Scholarship (2021–2026)</li>
                  <li>Received merit based full scholarship for complete Masters</li>
                  <li>Nominated for the Two Academic Excellence Medals 2014 in Intermediate Studies</li>
                </ul>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <p style="text-align:center;font-size:small;">
                  Template credits: <a href="https://jonbarron.info/">Jon Barron</a>.<br>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
    </table>
  </body>
</html>


